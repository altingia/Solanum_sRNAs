import os
import subprocess
#import pandas as pd


#########################
## Pipeline configuration
##########################
configfile:"config.yaml"

wildcard_constraints:
  dataset="[Aa-Zz0-9]+"

# working directory (will be cleaned at the end)
WORKING_DIR = config["workdir"]

# Input files
FQ_DIR = config["fastqdir"]
SPIKES = config["refs"]["spikes"]
SPIKES_BASENAME = os.path.basename(SPIKES)

# Reference fasta files and basename (just the fasta filename)
VIRUS = config["refs"]["virus"]
RFAM = config["refs"]["rfam"]
TRANSCRIPTOME = config["refs"]["transcriptome"]

###########
## Programs
###########
# Bowtie2 parameters
BOWTIE2_PARAMS_SPIKES = " ".join(config["bowtie2"]["spikes_params"].values())
BOWTIE2_PARAMS = " ".join(config["bowtie2"]["general_params"].values())

# Picard tools
SamToFastq = config["sam2fastq"]

# ShortStack parameters
SHORTSTACK_PARAMS = " ".join(config["shortstack"].values())

# EMBOSS einverted params
EINVERTED_PARAMS = config["einverted_params"]

###################
## Other parameters
###################

# read length parameters for trimming
MIN_LEN = config["trim"]["min_length"]
MAX_LEN = config["trim"]["max_length"]

# Threads
THREADS = 10

####################
## Desired outputs
####################

# Lengths
LENGTHS = expand("results/distri/{sample}.{trim}.txt",sample=config["samples"],trim=["original","trimmed"])

# Counts
SPIKES_COUNTS = expand("results/spikes/{sample}.txt",sample=config["samples"])
SEQCOUNTS = expand("results/counts/{sample}.counts.txt",sample=config["samples"])

# Inverted repeats
INVERT_REPEATS = expand(WORKING_DIR + "shortstack/einvert/{genome}_inverted_repeats.inv",genome=["LA0716","Heinz","LA2157","LYC4"])

# ShortStack
SHORTSTACK = expand("results/shortstack/{sample}/Results.txt",sample=config["samples"])

# Snakefile and config file used
MASTER_FILES = ["results/Snakefile","results/config.yaml"]

rule all: 
	input: 		
		SPIKES,	
		SPIKES_COUNTS,
		LENGTHS,	
		SEQCOUNTS,
		SHORTSTACK,
		MASTER_FILES
	message:"All done!"
     
#####################
## Copy master files
####################
rule copy_master_files:
    input:
        "Snakefile",
        "config.yaml"
    output: 
        "results/Snakefile",
        "results/config.yaml"
    shell:
        "cp {input} results/"

#####################
## Concatenate counts
#####################
rule concatenate_counts:
    input:
        "results/counts/{sample}.01_original.txt",
        "results/counts/{sample}.02_spikes.txt",
        "results/counts/{sample}.03_trimmed.txt",
        "results/counts/{sample}.04_virus.txt",
        "results/counts/{sample}.05_ncRNA.txt",
        "results/counts/{sample}.06_ShortStack_Heinz.txt"
    output:
        "results/counts/{sample}.counts.txt"
    message:"concatenating counts at all steps for {wildcards.sample}"
    shell:"cat {input} >> {output}"

#########################################################################
## Intersection with known miRNA (from miRNA gff3 genome annotation file)
#########################################################################

        
####################################################
## Shortstack analysis based on Solanum lycopersicum
####################################################
rule count_unmapped_after_ShortStack:
    input:
        bam = "results/shortstack/{sample}/{sample}_unaligned.bam"
    output:
        "results/counts/{sample}.06_ShortStack_Heinz.txt"
    message:"counting {wildcards.sample} sequences after ShortStack alignment"
    shell:"""
         count=$(($(samtools view -f 4 -c {input.bam})))
         echo "{wildcards.sample}\t06_ShortStack_Heinz\t$count" >> {output} 
         """

rule shortstack:
    input:
        reads =  WORKING_DIR + "ncRNA/{sample}_unaligned.fastq",
        genome = config["refs"]["genomes"]["pangenome"]
    output:
        "results/shortstack/{sample}/Results.txt",
        "results/shortstack/{sample}/{sample}_unaligned.bam"
    message:"Shortstack analysis of {wildcards.sample} using pangenome"
    version:"3.6"
    params:"results/shortstack/{sample}/"
    shell:
        "ShortStack "
        "--outdir {wildcards.sample} "
        "--bowtie_cores {THREADS} "
        "--sort_mem 4G "
        "--nostitch "	
        "{SHORTSTACK_PARAMS} "
        "--readfile {input.reads} "
        "--genome {input.genome};"
        "cp -r {wildcards.sample}/* {params};"
        "rm -r {wildcards.sample}"

#################################################################################
## Removal of non-coding RNAs (rRNA, tRNA, snoRNA) using non-coding RNA databases
#################################################################################
rule count_ncRNA:
    input:
        WORKING_DIR + "ncRNA/{sample}_unaligned.fastq"
    output:
        "results/counts/{sample}.05_ncRNA.txt"
    message:"counting {wildcards.sample} sequences after alignment to non-coding RNA sequences"
    shell:
        "python ../../scripts/count_seqs_in_fastq.py "
        "-f {input} -o {output} -f1 {wildcards.sample} -f2 05.ncRNA"

rule map_to_ncRNAs:
    input:
        reads = WORKING_DIR + "virus/{sample}_unaligned.fastq",
        index = [WORKING_DIR + "index/ncRNA" + x for x in [".1.bt2",".2.bt2",".3.bt2",".4.bt2",".rev.1.bt2",".rev.2.bt2"]]
    output:
        aln = WORKING_DIR + "ncRNA/{sample}_aln.sam",
	un =  WORKING_DIR + "ncRNA/{sample}_unaligned.fastq"
    message:"mapping {wildcards.sample} reads to non-coding RNA database"
    log:"results/logs/{sample}_aln2ncRNA.txt"
    params:
        WORKING_DIR + "index/ncRNA"
    shell:
        "bowtie2 {BOWTIE2_PARAMS} -p {THREADS} -x {params} --un {output.un} -U {input.reads} -S {output.aln} 2>{log}"

rule make_ncRNA_index:
    input:
        WORKING_DIR + "ncRNA/ncRNA.fasta"
    output:
        [WORKING_DIR + "index/ncRNA" + x for x in [".1.bt2",".2.bt2",".3.bt2",".4.bt2",".rev.1.bt2",".rev.2.bt2"]]
    message:"creating bowtie2 index for {input}"
    version:"2.1.0"
    shell:"bowtie2-build -q {input} " + WORKING_DIR + "index/ncRNA"

rule merge_RFAM_TIGR_repeats:
    input:
        tigr = config["refs"]["tigr"]["rDNA"],
        rfam = WORKING_DIR + "ncRNA/RFAM.parsed.fasta"
    output: 
        WORKING_DIR + "ncRNA/ncRNA.fasta"
    message:"merging RFAM and TIGR repeats (only ribosomal DNA) databases"
    shell:"cat {input} >> {output}"
          
rule parse_RFAM_db:
    input:
        WORKING_DIR + "ncRNA/RFAM.fa"
    output:
        WORKING_DIR + "ncRNA/RFAM.parsed.fasta"
    message:"keeping records for Solanum lycopersicum species"
    shell:
        "cat {input} |grep -A1 'Solanum lycopersicum' | " # this keeps records for tomato
        "sed -e '/microRNA/,+1d' > {output}" 		   # this removes the line and the following line when it matches microRNA

rule get_RFAM_database:
    output:
        rfam = WORKING_DIR + "ncRNA/RFAM.fa"
    message:"downloading last release of RFAM database"
    params:
        rfamlink = config["refs"]["rfam"],
    shell:
        "wget -r --quiet {params};"
        "gunzip --force ftp.ebi.ac.uk/pub/databases/Rfam/12.0/fasta_files/RF*.gz;"
        "cat  ftp.ebi.ac.uk/pub/databases/Rfam/12.0/fasta_files/*.fa >> {output}"
        

###########################################
## removal of plant virus from all samples
##########################################
rule count_virus:
    input:
        WORKING_DIR + "virus/{sample}_unaligned.fastq"
    output:
        "results/counts/{sample}.04_virus.txt"
    message:"counting {wildcards.sample} sequences after alignment to virus sequences"
    shell:
        "python ../../scripts/count_seqs_in_fastq.py "
        "-f {input} -o {output} -f1 {wildcards.sample} -f2 04.virus"

rule map_to_plant_virus:
    input: 
        index = ["index/virus" + x for x in [".1.bt2",".2.bt2",".3.bt2",".4.bt2",".rev.1.bt2",".rev.2.bt2"]],
        reads =  WORKING_DIR + "trim/{sample}.trimmed.fastq" 
    output:
        aln = WORKING_DIR + "virus/{sample}_aln.sam",
        un = WORKING_DIR + "virus/{sample}_unaligned.fastq"
    log: "results/logs/{sample}_aln2virus.txt"
    message:"mapping {wildcards.sample} reads to plant virus sequences" 
    version: "2.1.0"
    params:"index/virus"
    shell:"bowtie2 {BOWTIE2_PARAMS} -p {THREADS} -x {params} --un {output.un} -U {input.reads} -S {output.aln} 2>{log}"

rule make_virus_index:
    input:
        virus = config["refs"]["virus"]
    output:
        ["index/virus" + x for x in [".1.bt2",".2.bt2",".3.bt2",".4.bt2",".rev.1.bt2",".rev.2.bt2"]]
    message:"creating bowtie2 index for {VIRUS}"
    version:"2.1.0"
    shell:"bowtie2-build -q {input} index/virus"

##############################################
## Trim reads for all samples + counts/lengths
##############################################
rule trimmed_seq_lengths:
    input:
        WORKING_DIR + "trim/{sample}.trimmed.fastq"
    output:
        "results/distri/{sample}.trimmed.txt"
    message:"computing trimmed read length distribution for {wildcards.sample}"
    shell:
        "python ../../scripts/read_length_distribution.py -f {input} -o {output} -n {wildcards.sample}_trimmed"

rule count_trimmed:
    input:
        WORKING_DIR + "trim/{sample}.trimmed.fastq"
    output:
        "results/counts/{sample}.03_trimmed.txt"
    message:"counting {wildcards.sample} sequences after trimming"
    shell:
        "python ../../scripts/count_seqs_in_fastq.py "
        "-f {input} -o {output} -f1 {wildcards.sample} -f2 03.trimmed"

rule trim_reads:
    input:
        WORKING_DIR + "spikes/{sample}_unaligned.fastq"
    output:
        WORKING_DIR + "trim/{sample}.trimmed.fastq"
    message: "Trimming reads shorter than {0} and longer than {1}".format(MIN_LEN,MAX_LEN)
    shell:"""
	  cat {input} | paste - - - - | awk 'length($2)  >= {MIN_LEN} && length($2) <= {MAX_LEN}' |sed 's/\\t/\\n/g' > {output}
	  """

##################
## Spikes analysis
##################
rule count_seqs_after_spikes:
    input:
        un =  WORKING_DIR + "spikes/{sample}_unaligned.fastq"
    output:
        "results/counts/{sample}.02_spikes.txt"
    message:"counting {wildcards.sample} original reads"
    shell:
        "python ../../scripts/count_seqs_in_fastq.py "
        "-f {input} -o {output} -f1 {wildcards.sample} -f2 02.spikes"

rule counts_from_spikes:
    input:
        WORKING_DIR + "spikes/{sample}_spike_aln.sorted.bam",
        WORKING_DIR + "spikes/{sample}_spike_aln.sorted.bam.bai"
    output:
        "results/spikes/{sample}.txt"
    message:"computing counts on spike-ins for {wildcards.sample}"
    shell:"samtools idxstats {input} > {output}"

rule index_spikebam:
    input:
        WORKING_DIR + "spikes/{sample}_spike_aln.bam"
    output:
        bam  = WORKING_DIR + "spikes/{sample}_spike_aln.sorted.bam", 
        bai = WORKING_DIR + "spikes/{sample}_spike_aln.sorted.bam.bai"
    message:"indexing {input}"
    params: WORKING_DIR + "spikes/{sample}_spike_aln.sorted"
    shell:
        "samtools sort {input} {params};"
        "samtools index {output.bam}"

rule convert2bam:
    input:
        WORKING_DIR + "spikes/{sample}_spike_aln.sam"
    output:
        temp(WORKING_DIR + "spikes/{sample}_spike_aln.bam")
    message:"converting {input} to BAM"
    shell:"samtools view -b {input} > {output}"

rule aln2spikes:
    input:
        fq= FQ_DIR + "{sample}.fastq",
        index = ["index/spikes" + x for x in [".1.bt2",".2.bt2",".3.bt2",".4.bt2",".rev.1.bt2",".rev.2.bt2"]]
    output:
        aln = temp(WORKING_DIR + "spikes/{sample}_spike_aln.sam"),
        un =  WORKING_DIR + "spikes/{sample}_unaligned.fastq"
    message: "Aligning {wildcards.sample} reads to spike sequences."
    log:"results/logs/{sample}_spikes.txt"
    version: "2.1.0"
    params:"index/spikes"
    shell:
        "bowtie2 {BOWTIE2_PARAMS_SPIKES} -p {THREADS} -x {params} --un {output.un} -U {input.fq} -S {output.aln} 2>{log}"

rule make_spike_index:
    input:
        spikes = config["refs"]["spikes"]
    output:
        ["index/spikes" + x for x in [".1.bt2",".2.bt2",".3.bt2",".4.bt2",".rev.1.bt2",".rev.2.bt2"]]
    message:"creating bowtie2 index for {SPIKES}"
    version:"2.1.0"
    shell:
        "bowtie2-build -q {SPIKES} index/spikes"

#####################################################
## Output the read lengths in the different samples
##################################################### 
rule original_seq_lengths:
    input:
        fq= FQ_DIR + "{sample}.fastq"
    output:
        "results/distri/{sample}.original.txt"
    message:"computing original read length distribution for {wildcards.sample}"
    shell:
        "python ../../scripts/read_length_distribution.py -f {input} -o {output} -n {wildcards.sample}_original"

#####################################################
## Count number of sequences in the different samples
##################################################### 
rule count_original_seqs:
    input:
        fq= FQ_DIR + "{sample}.fastq"
    output:
        "results/counts/{sample}.01_original.txt"
    message:"counting {wildcards.sample} original reads"
    shell:
        "python ../../scripts/count_seqs_in_fastq.py "
        "-f {input} -o {output} -f1 {wildcards.sample} -f2 01.original"

