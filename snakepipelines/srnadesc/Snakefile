import os
import subprocess
#import pandas as pd


#########################
## Pipeline configuration
##########################
configfile:"config.yaml"

wildcard_constraints:
  dataset="[Aa-Zz0-9]+"

# directories 
FQ_DIR = config["fastqdir"]
WORKING_DIR = config["workdir"] 
RES_DIR = config["resultdir"]

# internal standards 
SPIKES = config["refs"]["spikes"]
SPIKES_BASENAME = os.path.basename(SPIKES)

# Reference fasta files and basename (just the fasta filename)
VIRUS = config["refs"]["virus"]
MITO = config["refs"]["mitochondria"]
CHLORO = config["refs"]["chloroplast"]
TRANSCRIPTOME = config["refs"]["transcriptome"]
CHROM_SIZES = config["refs"]["chromsizes"]
GENOME = config["refs"]["genome"]

###########
## Programs
###########
# Bowtie2 parameters
BOWTIE2_PARAMS_SPIKES = " ".join(config["bowtie2"]["spikes_params"].values())
BOWTIE2_PARAMS = " ".join(config["bowtie2"]["general_params"].values())

# ShortStack parameters
SHORTSTACK_PARAMS = " ".join(config["shortstack"].values())

###################
## Other parameters
###################

# read length parameters for trimming
MIN_LEN = config["trim"]["min_length"]
MAX_LEN = config["trim"]["max_length"]

# Threads
THREADS = config["threads"]

####################
## Desired outputs
####################

# Lengths
LENGTHS = expand(RES_DIR + "distri/{sample}.{trim}.txt",sample=config["samples"],trim=["original","trimmed"])

# Counts
SPIKES_COUNTS = expand(RES_DIR + "spikes/{sample}.txt",sample=config["samples"])
SEQCOUNTS = expand(RES_DIR + "counts/{sample}.counts.txt",sample=config["samples"])
NRSEQ_COUNTS = expand(RES_DIR + "nrcounts/{sample}.txt",sample=config["samples"])

# ShortStack
SHORTSTACK = expand(RES_DIR + "shortstack/{sample}/Results.txt",sample=config["samples"])
BIGWIGS = expand(RES_DIR + "bigwig/{sample}.bw",sample=config["samples"])

# non-redundant miRNAs
MIRNAS = RES_DIR + "miRNAs.fasta"

# Snakefile and config file used
MASTER_FILES = [RES_DIR + "Snakefile",RES_DIR + "config.yaml"]


rule all: 
	input: 		
		SPIKES,	
		SPIKES_COUNTS,
		LENGTHS,	
		SEQCOUNTS,
		NRSEQ_COUNTS,
		SHORTSTACK,
		MIRNAS,
		BIGWIGS,
		MASTER_FILES
	message:"All done!"
     
#####################
## Copy master files
####################
rule copy_master_files:
    input:
        "Snakefile",
        "config.yaml"
    output: 
        RES_DIR + "Snakefile",
        RES_DIR + "config.yaml"
    shell:
        "cp {input} results/"

#############################
## Merge all identified miRNA 
#############################

rule merge_all_identified_miRNAs:
    input:
        expand(RES_DIR + "shortstack/{sample}/Results.txt",sample=config["samples"])
    output:
        RES_DIR + "miRNAs.fasta"
    message:"Making a non-redundant fasta file of all detected miRNAs"
    shell:
         "python ../../scripts/merge_mirnas_from_shortstack.py -l {input} -f {output}"

#####################
## Concatenate counts
#####################
rule concatenate_counts:
    input:
        RES_DIR + "counts/{sample}.01_original.txt",
        RES_DIR + "counts/{sample}.02_spikes.txt",
        RES_DIR + "counts/{sample}.03_trimmed.txt",
        RES_DIR + "counts/{sample}.04_virus.txt",
        RES_DIR + "counts/{sample}.05_mitochondria.txt",
        RES_DIR + "counts/{sample}.06_chloroplast.txt",
        RES_DIR + "counts/{sample}.07_ncRNA.txt",
        RES_DIR + "counts/{sample}.08_ShortStack.txt"
    output:
        RES_DIR + "counts/{sample}.counts.txt"
    message:"concatenating counts at all steps for {wildcards.sample}"
    shell:
        "cat {input} >> {output}"
              

######################
## Shortstack analysis 
######################
rule bedgraph2bigwig:
    input:
        bedgraph = RES_DIR + "bigwig/{sample}.bedgraph",
        sizes = CHROM_SIZES
    output:
        RES_DIR + "bigwig/{sample}.bw"
    message:"generating {wildcards.sample} BigWig file" 
    params:
        RES_DIR + "bigwig/"
    shell:
        """
        for f in {input.bedgraph};
        do
            echo "converting $f file to bigwig format"
            samplename=$(basename $f .bedgraph)
            echo $samplename
            bedGraphToBigWig $f {CHROM_SIZES} $samplename.bw
            mv $samplename.bw {params}$samplename.bw
        done
        """

rule copy_chromosome_sizes:
    input:
        CHROM_SIZES
    output:
        temp("chromsizes.tab")
    shell:"cp {input} {output}"

rule bam2bedgraph:
    input:
        bam = RES_DIR + "shortstack/{sample}/{sample}_aligned.bam"
    output:
        RES_DIR + "bigwig/{sample}.bedgraph"
    message:"converting {wildcards.sample} to bedgraph"
    params:
        "{sample}.bg"
    shell:
        "bedtools genomecov -bg -ibam {input} > {params};"
        "bedSort {params} {output};"
        "rm {params}"

rule count_unmapped_after_ShortStack:
    input:
        bam = RES_DIR + "shortstack/{sample}/{sample}_aligned.bam"
    output:
        RES_DIR + "counts/{sample}.08_ShortStack.txt"
    message:"counting {wildcards.sample} sequences after ShortStack alignment"
    shell:"""
         samtools index {input};
         count=$(($(samtools view -f 4 -c {input.bam})))
         nrcount=$(($(samtools view -f 4 {input.bam} |awk '{{print $10}}' |sort|uniq|wc -l)))
         echo "{wildcards.sample}\t08_ShortStack\t$count\t$nrcount" >> {output} 
         """

rule rename_bams:
    input:
        RES_DIR + "shortstack/{sample}/{sample}_unaligned.bam"
    output:
        RES_DIR + "shortstack/{sample}/{sample}_aligned.bam"
    message:"renaming shortstack BAM files for {wildcards.sample}"
    shell:"mv {input} {output}"

rule shortstack:
    input:
        reads =  WORKING_DIR + "ncRNA/{sample}_unaligned.fastq",
        genome = GENOME
    output:
        RES_DIR + "shortstack/{sample}/Results.txt",
        RES_DIR + "shortstack/{sample}/{sample}_unaligned.bam"
    message:"Shortstack analysis of {wildcards.sample} using {input.genome} reference"
    params:
        RES_DIR + "shortstack/{sample}/"
    shell:
        "ShortStack "
        "--outdir {wildcards.sample} "
        "--bowtie_cores {THREADS} "
        "--sort_mem 4G "
        "{SHORTSTACK_PARAMS} "
        "--readfile {input.reads} "
        "--genome {input.genome};"
        "cp -r {wildcards.sample}/* {params};"
        "rm -r {wildcards.sample};"

#################################################################################
## Removal of non-coding RNAs (rRNA, tRNA, snoRNA) using non-coding RNA databases
#################################################################################
rule count_ncRNA:
    input:
        WORKING_DIR + "ncRNA/{sample}_unaligned.fastq"
    output:
        RES_DIR + "counts/{sample}.07_ncRNA.txt"
    message:"counting {wildcards.sample} sequences after alignment to non-coding RNA sequences"
    shell:
        "python ../../scripts/count_seqs_in_fastq.py "
        "-f {input} -o {output} -f1 {wildcards.sample} -f2 07.ncRNA"

rule map_to_ncRNAs:
    input:
        reads = WORKING_DIR + "chloroplast/{sample}_unaligned.fastq",
        index = [WORKING_DIR + "index/ncRNA" + x for x in [".1.bt2",".2.bt2",".3.bt2",".4.bt2",".rev.1.bt2",".rev.2.bt2"]]
    output:
        aln = WORKING_DIR + "ncRNA/{sample}_aln.sam",
	un =  WORKING_DIR + "ncRNA/{sample}_unaligned.fastq"
    message:"mapping {wildcards.sample} reads to non-coding RNA database"
    log:RES_DIR + "logs/{sample}_aln2ncRNA.txt"
    params:
        WORKING_DIR + "index/ncRNA"
    shell:
        "bowtie2 {BOWTIE2_PARAMS} -p {THREADS} -x {params} --un {output.un} -U {input.reads} -S {output.aln} 2>{log}"

rule make_ncRNA_index:
    input:
        config["refs"]["ncrna"]
    output:
        [WORKING_DIR + "index/ncRNA" + x for x in [".1.bt2",".2.bt2",".3.bt2",".4.bt2",".rev.1.bt2",".rev.2.bt2"]]
    message:"creating bowtie2 index for {input}"
    version:"2.1.0"
    params:
        WORKING_DIR + "index/ncRNA"
    shell:"bowtie2-build -q {input} {params}"
      
################################
## Removal of chloroplastic RNAs
################################
rule count_chloroplast:
    input:
        WORKING_DIR + "chloroplast/{sample}_unaligned.fastq"
    output:
        RES_DIR + "counts/{sample}.06_chloroplast.txt"
    message:"counting {wildcards.sample} sequences after alignment to virus sequences"
    shell:
        "python ../../scripts/count_seqs_in_fastq.py "
        "-f {input} -o {output} -f1 {wildcards.sample} -f2 06.chloroplast"

rule map_to_chloroplast:
    input: 
        index = [WORKING_DIR + "index/chloroplast" + x for x in [".1.bt2",".2.bt2",".3.bt2",".4.bt2",".rev.1.bt2",".rev.2.bt2"]],
        reads =  WORKING_DIR + "mitochondria/{sample}_unaligned.fastq" 
    output:
        aln = WORKING_DIR + "chloroplast/{sample}_aln.sam",
        un = WORKING_DIR + "chloroplast/{sample}_unaligned.fastq"
    log: RES_DIR + "logs/{sample}_aln2chloroplast.txt"
    message:"mapping {wildcards.sample} reads to chloroplastl genome" 
    version: "2.1.0"
    params:
        WORKING_DIR + "index/chloroplast"
    shell:"bowtie2 {BOWTIE2_PARAMS} -p {THREADS} -x {params} --un {output.un} -U {input.reads} -S {output.aln} 2>{log}"

rule make_chloroplastl_index:
    input:
        mito = config["refs"]["chloroplast"]
    output:
        [WORKING_DIR + "index/chloroplast" + x for x in [".1.bt2",".2.bt2",".3.bt2",".4.bt2",".rev.1.bt2",".rev.2.bt2"]]
    message:"creating bowtie2 index for chloroplast genome"
    version:"2.1.0"
    params:
        WORKING_DIR + "index/chloroplast"
    shell:"bowtie2-build -q {input} {params}"

################################
## Removal of mitochondrial RNAs
################################
rule count_mitochondria:
    input:
        WORKING_DIR + "mitochondria/{sample}_unaligned.fastq"
    output:
        RES_DIR + "counts/{sample}.05_mitochondria.txt"
    message:"counting {wildcards.sample} sequences after alignment to virus sequences"
    shell:
        "python ../../scripts/count_seqs_in_fastq.py "
        "-f {input} -o {output} -f1 {wildcards.sample} -f2 05.mitochondria"

rule map_to_mitochondria:
    input: 
        index = [WORKING_DIR + "index/mitochondria" + x for x in [".1.bt2",".2.bt2",".3.bt2",".4.bt2",".rev.1.bt2",".rev.2.bt2"]],
        reads =  WORKING_DIR + "virus/{sample}_unaligned.fastq" 
    output:
        aln = WORKING_DIR + "mitochondria/{sample}_aln.sam",
        un = WORKING_DIR + "mitochondria/{sample}_unaligned.fastq"
    log: RES_DIR + "logs/{sample}_aln2mitochondria.txt"
    message:"mapping {wildcards.sample} reads to mitochondrial genome" 
    version: "2.1.0"
    params:
        WORKING_DIR + "index/mitochondria"
    shell:"bowtie2 {BOWTIE2_PARAMS} -p {THREADS} -x {params} --un {output.un} -U {input.reads} -S {output.aln} 2>{log}"

rule make_mitochondrial_index:
    input:
        mito = config["refs"]["mitochondria"]
    output:
        [WORKING_DIR + "index/mitochondria" + x for x in [".1.bt2",".2.bt2",".3.bt2",".4.bt2",".rev.1.bt2",".rev.2.bt2"]]
    message:"creating bowtie2 index for mitochondrial genome"
    version:"2.1.0"
    params:
        WORKING_DIR + "index/mitochondria"
    shell:"bowtie2-build -q {input} {params}"

###########################################
## removal of plant virus from all samples
##########################################
rule count_virus:
    input:
        WORKING_DIR + "virus/{sample}_unaligned.fastq"
    output:
        RES_DIR + "counts/{sample}.04_virus.txt"
    message:"counting {wildcards.sample} sequences after alignment to virus sequences"
    shell:
        "python ../../scripts/count_seqs_in_fastq.py "
        "-f {input} -o {output} -f1 {wildcards.sample} -f2 04.virus"

rule map_to_plant_virus:
    input: 
        index = [WORKING_DIR + "index/virus" + x for x in [".1.bt2",".2.bt2",".3.bt2",".4.bt2",".rev.1.bt2",".rev.2.bt2"]],
        reads =  WORKING_DIR + "trim/{sample}.trimmed.fastq" 
    output:
        aln = WORKING_DIR + "virus/{sample}_aln.sam",
        un = WORKING_DIR + "virus/{sample}_unaligned.fastq"
    log: RES_DIR + "logs/{sample}_aln2virus.txt"
    message:"mapping {wildcards.sample} reads to plant virus sequences" 
    version: "2.1.0"
    params:
        WORKING_DIR + "index/virus"
    shell:"bowtie2 {BOWTIE2_PARAMS} -p {THREADS} -x {params} --un {output.un} -U {input.reads} -S {output.aln} 2>{log}"

rule make_virus_index:
    input:
        virus = config["refs"]["virus"]
    output:
        [WORKING_DIR + "index/virus" + x for x in [".1.bt2",".2.bt2",".3.bt2",".4.bt2",".rev.1.bt2",".rev.2.bt2"]]
    message:"creating bowtie2 index for {VIRUS}"
    version:"2.1.0"
    params:
        WORKING_DIR + "index/virus"
    shell:"bowtie2-build -q {input} {params}"

##############################################
## Trim reads for all samples + counts/lengths
##############################################
rule trimmed_seq_lengths:
    input:
        WORKING_DIR + "trim/{sample}.trimmed.fastq"
    output:
        RES_DIR + "distri/{sample}.trimmed.txt"
    message:"computing trimmed read length distribution for {wildcards.sample}"
    shell:
        "python ../../scripts/read_length_distribution.py -f {input} -o {output} -n {wildcards.sample}_trimmed"

rule count_trimmed:
    input:
        WORKING_DIR + "trim/{sample}.trimmed.fastq"
    output:
        RES_DIR + "counts/{sample}.03_trimmed.txt"
    message:"counting {wildcards.sample} sequences after trimming"
    shell:
        "python ../../scripts/count_seqs_in_fastq.py "
        "-f {input} -o {output} -f1 {wildcards.sample} -f2 03.trimmed"

rule trim_reads:
    input:
        WORKING_DIR + "spikes/{sample}_unaligned.fastq"
    output:
        WORKING_DIR + "trim/{sample}.trimmed.fastq"
    message: "Trimming reads shorter than {0} and longer than {1}".format(MIN_LEN,MAX_LEN)
    shell:"""
	  cat {input} | paste - - - - | awk 'length($2)  >= {MIN_LEN} && length($2) <= {MAX_LEN}' |sed 's/\\t/\\n/g' > {output}
	  """

##################
## Spikes analysis
##################
rule count_seqs_after_spikes:
    input:
        un =  WORKING_DIR + "spikes/{sample}_unaligned.fastq"
    output:
        RES_DIR + "counts/{sample}.02_spikes.txt"
    message:"counting {wildcards.sample} original reads"
    shell:
        "python ../../scripts/count_seqs_in_fastq.py "
        "-f {input} -o {output} -f1 {wildcards.sample} -f2 02.spikes"

rule counts_from_spikes:
    input:
        WORKING_DIR + "spikes/{sample}_spike_aln.sorted.bam",
        WORKING_DIR + "spikes/{sample}_spike_aln.sorted.bam.bai"
    output:
        RES_DIR + "spikes/{sample}.txt"
    message:"computing counts on spike-ins for {wildcards.sample}"
    shell:"samtools idxstats {input} > {output}"

rule index_spikebam:
    input:
        WORKING_DIR + "spikes/{sample}_spike_aln.bam"
    output:
        bam  = WORKING_DIR + "spikes/{sample}_spike_aln.sorted.bam", 
        bai = WORKING_DIR + "spikes/{sample}_spike_aln.sorted.bam.bai"
    message:"indexing {input}"
    params: WORKING_DIR + "spikes/{sample}_spike_aln.sorted"
    shell:
        "samtools sort {input} {params};"
        "samtools index {output.bam}"

rule convert2bam:
    input:
        WORKING_DIR + "spikes/{sample}_spike_aln.sam"
    output:
        temp(WORKING_DIR + "spikes/{sample}_spike_aln.bam")
    message:"converting {input} to BAM"
    shell:"samtools view -b {input} > {output}"

rule aln2spikes:
    input:
        fq= FQ_DIR + "{sample}.fastq",
        index = [WORKING_DIR + "index/spikes" + x for x in [".1.bt2",".2.bt2",".3.bt2",".4.bt2",".rev.1.bt2",".rev.2.bt2"]]
    output:
        aln = temp(WORKING_DIR + "spikes/{sample}_spike_aln.sam"),
        un =  WORKING_DIR + "spikes/{sample}_unaligned.fastq"
    message: "Aligning {wildcards.sample} reads to spike sequences."
    log:
        RES_DIR + "logs/{sample}_spikes.txt"
    version: "2.1.0"
    params:
        WORKING_DIR + "index/spikes"
    shell:
        "bowtie2 {BOWTIE2_PARAMS_SPIKES} -p {THREADS} -x {params} --un {output.un} -U {input.fq} -S {output.aln} 2>{log}"

rule make_spike_index:
    input:
        spikes = config["refs"]["spikes"]
    output:
        [WORKING_DIR + "index/spikes" + x for x in [".1.bt2",".2.bt2",".3.bt2",".4.bt2",".rev.1.bt2",".rev.2.bt2"]]
    message:"creating bowtie2 index for {SPIKES}"
    version:"2.1.0"
    params:
        WORKING_DIR + "index/spikes"
    shell:
        "bowtie2-build -q {SPIKES} {params}"

#####################################################
## Output the read lengths in the different samples
##################################################### 
rule original_seq_lengths:
    input:
        fq= FQ_DIR + "{sample}.fastq"
    output:
        RES_DIR + "distri/{sample}.original.txt"
    message:"computing original read length distribution for {wildcards.sample}"
    shell:
        "python ../../scripts/read_length_distribution.py -f {input} -o {output} -n {wildcards.sample}_original"

#####################################################
## Count number of sequences in the different samples
##################################################### 
rule count_original_seqs:
    input:
        fq= FQ_DIR + "{sample}.fastq"
    output:
        RES_DIR + "counts/{sample}.01_original.txt"
    message:"counting {wildcards.sample} original reads"
    shell:
        "python ../../scripts/count_seqs_in_fastq.py "
        "-f {input} -o {output} -f1 {wildcards.sample} -f2 01.original"

rule count_nr_sequences:
    input:
        fq= FQ_DIR + "{sample}.fastq"
    output:
        RES_DIR + "nrcounts/{sample}.txt"
    shell:
        "python ../../scripts/count_nr_sequence_in_fastq.py "
        "-i {input} -o {output} "
